# -*- coding: utf-8 -*-
"""fcc_sms_text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1BAUjFZ7uoQylVpE2ya90apsP3WkjX3
"""

import tensorflow as tf
import pandas as pd
from tensorflow import keras
!pip install tensorflow-datasets
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow import keras
# import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import sequence
tfds.disable_progress_bar()

# get data files
!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv

train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

import pandas as pd
import nltk
from nltk.corpus import stopwords
import string
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
traindata=pd.read_csv(train_file_path, sep="\t", header=None, names=["type", "msg"])
testdata= pd.read_csv(test_file_path, sep="\t", header=None, names=["type", "msg"])

traindata.head()

testdata.head()

def pr(x):
   x=x.lower()
   tokens=nltk.word_tokenize(x)
   stop_words=set(stopwords.words('english'))
   punctuations = string.punctuation
   tagged_tokens = nltk.pos_tag(tokens)
   lemmatizer = WordNetLemmatizer()
   lemmas = []
   for token, tag in tagged_tokens:
     if token not in stop_words and token not in punctuations:
        pos = tag[0].lower() if tag[0].lower() in ['a', 'r', 'n', 'v'] else wordnet.NOUN
        lemma = lemmatizer.lemmatize(token, pos=pos)
        lemmas.append(lemma)

   filtered_sentence = ' '.join(lemmas)
   return filtered_sentence

traindata['msg'] = traindata['msg'].apply(pr)
traindata.head()

testdata['msg'] = testdata['msg'].apply(pr)
testdata.head()

import matplotlib.pyplot as plt
from imblearn.under_sampling import RandomUnderSampler
traindata['type'].value_counts().plot(kind='bar')
rus = RandomUnderSampler(sampling_strategy='majority')
X_resampled, y_resampled = rus.fit_resample(traindata[['msg']], traindata[['type']])

y_resampled.value_counts().plot(kind='bar')

x_train=X_resampled['msg']
y_train=y_resampled['type']
print(x_train.shape)
print(y_train.shape)

x_train

y_train.value_counts()

label_mapper = lambda x: 1 if x == 'spam' else 0
y_train = y_train.map(label_mapper)

y_train

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit(x_train)
X_tfidf=vectorizer.transform(x_train)
x=X_tfidf

x.shape
x_train=x

df_train_tfidf = pd.DataFrame.sparse.from_spmatrix(x_train, columns=vectorizer.vocabulary_.keys())
df_train_tfidf

label_mapper = lambda x: 1 if x == 'spam' else 0
testdata['type'] = testdata['type'].map(label_mapper)

x_test=testdata[['msg']]
y_test=testdata['type']

y_test

X_tfidf_test = vectorizer.transform(testdata['msg'])
x_test=X_tfidf_test

df_test_tfidf = pd.DataFrame.sparse.from_spmatrix(x_test, columns=vectorizer.vocabulary_.keys())
df_test_tfidf

df_train_tfidf

y_train.value_counts()

print('x_train', x_train.shape)
print('y_train', y_train.shape)
print('x_test', x_test.shape)
print('y_test', y_test.shape)

from sklearn import svm
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
clf = svm.SVC(kernel='rbf')
clf.fit(x_train,y_train)
y_pred = clf.predict(x_train)
y_testp = clf.predict(x_test)
acc_score = accuracy_score(y_train, y_pred)
acc_scoret = accuracy_score(y_test, y_testp)
print('Accuracy score train :', acc_score)
print('Accuracy score test :', acc_scoret)

train=df_train_tfidf
test=df_test_tfidf

print(train.shape)
print(test.shape)

inputs = tf.keras.Input(shape=(None,train.shape[1]))
x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)

outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

train=np.array(train)
test=np.array(test)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=13)


h=model.fit(x=train,y=y_train,batch_size=32,validation_data=(test,y_test),epochs=100,callbacks=[early_stop],  validation_steps=30)

def plot_graphs(h, metric):
    plt.plot(h.history[metric])
    plt.plot(h.history['val_'+metric])
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend([metric, 'val_'+metric])

plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plot_graphs(h, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(h, 'loss')
plt.ylim(0, None)

traindata["type"] = pd.factorize(traindata["type"])[0]
testdata["type"] = pd.factorize(testdata["type"])[0]

train_labels =  traindata["type"].values
train_ds = tf.data.Dataset.from_tensor_slices(
    (traindata["msg"].values, train_labels)
)

test_labels =  testdata["type"].values
test_ds = tf.data.Dataset.from_tensor_slices(
    (testdata["msg"].values, test_labels)
)
test_ds.element_spec

"""This step is important. It's not only about batching the data sets, but also reshape it to make it works when fitting the model. Otherwise, we will get incompatible layers errors: expecting ndim=3, got ndim=2.a"""

BUFFER_SIZE = 100
BATCH_SIZE = 32
train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

vec = TextVectorization(
    output_mode='int',
    max_tokens=1000,
    output_sequence_length=1000,
)

vec.adapt(train_ds.map(lambda text, label: text))

vocab = np.array(vec.get_vocabulary())
vocab[:20]

model2 = tf.keras.Sequential([
    vec,
    tf.keras.layers.Embedding(
        len(vec.get_vocabulary()),
        64,
        mask_zero=True,
    ),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1)
])


model2.compile(
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(1e-4),
    metrics=['accuracy'],
)

history = model2.fit(
    train_ds,
    validation_data=test_ds,
    validation_steps=30,
    epochs=10,
)

plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.ylim(None, 1)
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')
plt.ylim(0, None)

from google.colab import drive
drive.mount('/content/drive')

# function to predict messages based on model
# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])

def predict_message(pred_text):
  pred_matrix_sparse = vectorizer.transform([pred_text])
  pred_matrix_dense = pred_matrix_sparse.toarray()
  yprediction = model.predict(pred_matrix_dense)


  if yprediction[0]<0.5:
    return [yprediction[0],'ham']
  else:
      return [y_pred[0],'spam']


pred_text = "sale today! to stop texts call 9891246032"

prediction = predict_message(pred_text)
print(prediction)

# Run this cell to test your function and model. Do not modify contents.
def test_predictions():
  test_messages = ["how are you doing today",
                   "sale today! to stop texts call 98912460324",
                   "i dont want to go. can we try it a different day? available sat",
                   "our new mobile video service is live. just install on your phone to start watching.",
                   "you have won Â£1000 cash! call to claim your prize.",
                   "i'll bring it tomorrow. don't forget the milk.",
                   "wow, is your arm alright. that happened to me one time too"
                  ]

  test_answers = ["ham", "spam", "ham", "spam", "spam", "ham", "ham"]
  passed = True

  for msg, ans in zip(test_messages, test_answers):
    prediction = predict_message(msg)
    if prediction[1] != ans:
      passed = False

  if passed:
    print("You passed the challenge. Great job!")
  else:
    print("You haven't passed yet. Keep trying.")

test_predictions()